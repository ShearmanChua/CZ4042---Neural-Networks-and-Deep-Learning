{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tfrecords_handler.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"code","metadata":{"id":"mev7bMZDxBLS"},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import math\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","AUTO = tf.data.experimental.AUTOTUNE\n","from PIL import Image\n","import os\n","import IPython.display as display"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ze8DxMvxBLd"},"source":["def _bytestring_feature(list_of_bytestrings):\n","    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n","\n","def _int_feature(list_of_ints): # int64\n","    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n","\n","def _float_feature(list_of_floats): # float32\n","    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciswma5LxBLe"},"source":["df = pd.read_csv('df.csv')\n","\n","# Train - 67.5%\n","# Val - 22.5%\n","# Test - 10%\n","\n","train_data = pd.read_csv('train.csv')\n","test_data = pd.read_csv('test.csv')\n","val_data = pd.read_csv('val.csv')\n","\n","# for i in range(len(test_data)):\n","#     row = test_data.iloc[i]\n","\n","#     if row.model_id not in train_data.model_id.unique():\n","#         test_data.at[i, 'model_id'] = 1\n","\n","#     if row.make_id not in train_data.make_id.unique():\n","#         test_data.at[i, 'make_id'] = 1\n","\n","label_encoder = LabelEncoder().fit(df.make_id.astype(str))\n","train_data.make_id = label_encoder.transform(train_data.make_id.astype(str))\n","label_encoder = LabelEncoder().fit(df.model_id.astype(str))\n","train_data.model_id = label_encoder.transform(train_data.model_id.astype(str))\n","\n","label_encoder = LabelEncoder().fit(df.make_id.astype(str))\n","val_data.make_id = label_encoder.transform(val_data.make_id.astype(str))\n","label_encoder = LabelEncoder().fit(df.model_id.astype(str))\n","val_data.model_id = label_encoder.transform(val_data.model_id.astype(str))\n","\n","label_encoder = LabelEncoder().fit(df.make_id.astype(str))\n","test_data.make_id = label_encoder.transform(test_data.make_id.astype(str))\n","label_encoder = LabelEncoder().fit(df.model_id.astype(str))\n","test_data.model_id = label_encoder.transform(test_data.model_id.astype(str))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtbF-_ZaTxIe"},"source":["train_image_paths = train_data['filename']\n","train_labels = train_data[['make_id', 'model_id']]\n","\n","val_image_paths = val_data['filename']\n","val_labels = val_data[['make_id', 'model_id']]\n","\n","test_image_paths = test_data['filename']\n","test_labels = test_data[['make_id', 'model_id']]\n","\n","tfrecord_train_dir = 'tfrecords/train/'\n","tfrecord_val_dir = 'tfrecords/val/'\n","tfrecord_test_dir = 'tfrecords/test/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BROdvPi0xBLg"},"source":["## TRAIN DATA"]},{"cell_type":"code","metadata":{"id":"OSkHEIgJxBLg","outputId":"a0881432-f0fd-4c05-b6e8-02ccf5655dba"},"source":["SHARDS = 128\n","nb_images = len(train_data)\n","shard_size = math.ceil(1.0 * nb_images / SHARDS)\n","print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Pattern matches 92201 images which will be rewritten as 128 .tfrec files containing 721 images each.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"G8tLLXXyxBLi"},"source":["def _parse_function(filename, label):\n","    img_raw = tf.io.read_file(filename)\n","    return img_raw, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6yzeI9bxBLj"},"source":["files = tf.data.Dataset.from_tensor_slices((train_image_paths, train_labels))\n","dataset = files.map(_parse_function)\n","dataset = dataset.batch(shard_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aXjI7hV2UnaB"},"source":["def to_tfrecord(tfrec_filewriter, img_bytes, label):\n","    one_hot_class = [np.eye(163)[label[0]], np.eye(1716)[label[1]]]\n","    \n","    feature = {\n","        \"image\": _bytestring_feature([img_bytes]), # one image in the list\n","        \"make_id\": _int_feature([label[0]]),\n","        \"make_id_oh\": _float_feature(one_hot_class[0].tolist()),\n","        \"model_id\": _int_feature([label[1]]),\n","        \"model_id_oh\": _float_feature(one_hot_class[1].tolist())\n","    }\n","    return tf.train.Example(features=tf.train.Features(feature=feature))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"gjDEknQLxBLk","outputId":"63d50d73-08d3-4abc-f4be-146305cc4df8"},"source":["print(\"Writing TFRecords\")\n","for shard, (image, label) in enumerate(dataset):\n","  # batch size used as shard size here\n","  shard_size = image.numpy().shape[0]\n","  # good practice to have the number of records in the filename\n","  filename = tfrecord_train_dir + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n","  \n","  with tf.io.TFRecordWriter(filename) as out_file:\n","    for i in range(shard_size):\n","        example = to_tfrecord(out_file,\n","                              image.numpy()[i],\n","                              label.numpy()[i])\n","        out_file.write(example.SerializeToString())\n","    \n","    print(\"Wrote file {} containing {} records\".format(filename, shard_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing TFRecords\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"index 1702 is out of bounds for axis 0 with size 1677","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-112-cf3f394968c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         example = to_tfrecord(out_file,\n\u001b[0m\u001b[1;32m     11\u001b[0m                               \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                               label.numpy()[i])\n","\u001b[0;32m<ipython-input-111-fb0698b066a2>\u001b[0m in \u001b[0;36mto_tfrecord\u001b[0;34m(tfrec_filewriter, img_bytes, label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_tfrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfrec_filewriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mone_hot_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m162\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1677\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     feature = {\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_bytestring_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_bytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# one image in the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 1702 is out of bounds for axis 0 with size 1677"]}]},{"cell_type":"markdown","metadata":{"id":"HVJDafBcxBLk"},"source":["## VALIDATION DATA"]},{"cell_type":"code","metadata":{"id":"CaEKQx7FxBLl","outputId":"10602e41-0c60-4f18-97ec-9e32d4b01f35"},"source":["SHARDS = 32\n","nb_images = len(val_data)\n","shard_size = math.ceil(1.0 * nb_images / SHARDS)\n","print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Pattern matches 30734 images which will be rewritten as 32 .tfrec files containing 961 images each.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dJblth4TxBLl"},"source":["# def _parse_function(filename, label):\n","#     img_raw = tf.io.read_file(filename)\n","#     return img_raw, label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-m6LpX2sxBLm"},"source":["files = tf.data.Dataset.from_tensor_slices((val_image_paths, val_labels))\n","dataset = files.map(_parse_function)\n","dataset = dataset.batch(shard_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZvmbbU7xBLm"},"source":["# def to_tfrecord(tfrec_filewriter, img_bytes, label):\n","#     one_hot_class = np.eye(42)[label] \n","#     feature = {\n","#         \"image\": _bytestring_feature([img_bytes]), # one image in the list\n","#         \"class\": _int_feature([label]),\n","#         \"one_hot_class\": _float_feature(one_hot_class.tolist())\n","#     }\n","#     return tf.train.Example(features=tf.train.Features(feature=feature))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mT7hQQOyxBLn","outputId":"b2199c4f-bb5f-4991-ea30-f150a0ebfa10"},"source":["print(\"Writing TFRecords\")\n","for shard, (image, label) in enumerate(dataset):\n","  # batch size used as shard size here\n","  shard_size = image.numpy().shape[0]\n","  # good practice to have the number of records in the filename\n","  filename = tfrecord_val_dir + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n","  \n","  with tf.io.TFRecordWriter(filename) as out_file:\n","    for i in range(shard_size):\n","        example = to_tfrecord(out_file,\n","                              image.numpy()[i],\n","                              label.numpy()[i])\n","        out_file.write(example.SerializeToString())\n","    \n","    print(\"Wrote file {} containing {} records\".format(filename, shard_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing TFRecords\n","Wrote file tfrecords/val/00-961.tfrec containing 961 records\n","Wrote file tfrecords/val/01-961.tfrec containing 961 records\n","Wrote file tfrecords/val/02-961.tfrec containing 961 records\n","Wrote file tfrecords/val/03-961.tfrec containing 961 records\n","Wrote file tfrecords/val/04-961.tfrec containing 961 records\n","Wrote file tfrecords/val/05-961.tfrec containing 961 records\n","Wrote file tfrecords/val/06-961.tfrec containing 961 records\n","Wrote file tfrecords/val/07-961.tfrec containing 961 records\n","Wrote file tfrecords/val/08-961.tfrec containing 961 records\n","Wrote file tfrecords/val/09-961.tfrec containing 961 records\n","Wrote file tfrecords/val/10-961.tfrec containing 961 records\n","Wrote file tfrecords/val/11-961.tfrec containing 961 records\n","Wrote file tfrecords/val/12-961.tfrec containing 961 records\n","Wrote file tfrecords/val/13-961.tfrec containing 961 records\n","Wrote file tfrecords/val/14-961.tfrec containing 961 records\n","Wrote file tfrecords/val/15-961.tfrec containing 961 records\n","Wrote file tfrecords/val/16-961.tfrec containing 961 records\n","Wrote file tfrecords/val/17-961.tfrec containing 961 records\n","Wrote file tfrecords/val/18-961.tfrec containing 961 records\n","Wrote file tfrecords/val/19-961.tfrec containing 961 records\n","Wrote file tfrecords/val/20-961.tfrec containing 961 records\n","Wrote file tfrecords/val/21-961.tfrec containing 961 records\n","Wrote file tfrecords/val/22-961.tfrec containing 961 records\n","Wrote file tfrecords/val/23-961.tfrec containing 961 records\n","Wrote file tfrecords/val/24-961.tfrec containing 961 records\n","Wrote file tfrecords/val/25-961.tfrec containing 961 records\n","Wrote file tfrecords/val/26-961.tfrec containing 961 records\n","Wrote file tfrecords/val/27-961.tfrec containing 961 records\n","Wrote file tfrecords/val/28-961.tfrec containing 961 records\n","Wrote file tfrecords/val/29-961.tfrec containing 961 records\n","Wrote file tfrecords/val/30-961.tfrec containing 961 records\n","Wrote file tfrecords/val/31-943.tfrec containing 943 records\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZmAGTCn7xBLo"},"source":["## TEST DATA"]},{"cell_type":"code","metadata":{"id":"DY5-b5DJxBLp","outputId":"e24b6e38-87cf-4d29-d27b-357ea86ac562"},"source":["SHARDS = 16\n","nb_images = len(test_data)\n","shard_size = math.ceil(1.0 * nb_images / SHARDS)\n","print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Pattern matches 13791 images which will be rewritten as 16 .tfrec files containing 862 images each.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7MQJLgn5xBLp"},"source":["# def _parse_function(filename):\n","#     img_raw = tf.io.read_file(filename)\n","#     return img_raw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zdx1lJaQxBLq"},"source":["files = tf.data.Dataset.from_tensor_slices((test_image_paths, test_labels))\n","dataset = files.map(_parse_function)\n","dataset = dataset.batch(shard_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AIengEP7xBLr"},"source":["# def to_tfrecord(tfrec_filewriter, img_bytes):\n","#     feature = {\n","#         \"image\": _bytestring_feature([img_bytes]), # one image in the list\n","#     }\n","#     return tf.train.Example(features=tf.train.Features(feature=feature))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhpqnLDzxBLr","outputId":"d3fce08c-681f-4340-bde9-897adf5e1842"},"source":["print(\"Writing TFRecords\")\n","for shard, (image, label) in enumerate(dataset):\n","  # batch size used as shard size here\n","  shard_size = image.numpy().shape[0]\n","  # good practice to have the number of records in the filename\n","  filename = tfrecord_test_dir + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n","  \n","  with tf.io.TFRecordWriter(filename) as out_file:\n","    for i in range(shard_size):\n","        example = to_tfrecord(out_file,\n","                              image.numpy()[i],\n","                              label.numpy()[i])\n","        out_file.write(example.SerializeToString())\n","    \n","    print(\"Wrote file {} containing {} records\".format(filename, shard_size))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Writing TFRecords\n","Wrote file tfrecords/test/00-862.tfrec containing 862 records\n","Wrote file tfrecords/test/01-862.tfrec containing 862 records\n","Wrote file tfrecords/test/02-862.tfrec containing 862 records\n","Wrote file tfrecords/test/03-862.tfrec containing 862 records\n","Wrote file tfrecords/test/04-862.tfrec containing 862 records\n","Wrote file tfrecords/test/05-862.tfrec containing 862 records\n","Wrote file tfrecords/test/06-862.tfrec containing 862 records\n","Wrote file tfrecords/test/07-862.tfrec containing 862 records\n","Wrote file tfrecords/test/08-862.tfrec containing 862 records\n","Wrote file tfrecords/test/09-862.tfrec containing 862 records\n","Wrote file tfrecords/test/10-862.tfrec containing 862 records\n","Wrote file tfrecords/test/11-862.tfrec containing 862 records\n","Wrote file tfrecords/test/12-862.tfrec containing 862 records\n","Wrote file tfrecords/test/13-862.tfrec containing 862 records\n","Wrote file tfrecords/test/14-862.tfrec containing 862 records\n","Wrote file tfrecords/test/15-861.tfrec containing 861 records\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mtspSwdFxBLs"},"source":["## READ TRAIN/VAL TFRECORDS"]},{"cell_type":"code","metadata":{"id":"GQknpWdDxBLs"},"source":["IMAGE_SIZE = [224,224]\n","BATCH_SIZE = 128\n","\n","def read_tfrecord(example):\n","    features = {\n","        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n","        \"make_id\": tf.io.FixedLenFeature([], tf.int64),   # shape [] means scalar\n","        \"make_id_oh\": tf.io.VarLenFeature(tf.float32) # a certain number of floats\n","        \"model_id\": tf.io.FixedLenFeature([], tf.int64),   # shape [] means scalar\n","        \"model_id_oh\": tf.io.VarLenFeature(tf.float32)# a certain number of floats\n","    }\n","    \n","    feature = tf.io.parse_single_example(example, features)\n","    image = tf.image.decode_jpeg(feature['image'], channels=3)\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","    image = tf.image.resize(image, [*IMAGE_SIZE])\n","    label = feature['class']\n","    one_hot_class = tf.sparse.to_dense(feature['one_hot_class'])\n","    one_hot_class = tf.reshape(one_hot_class, [42])\n","    return image, one_hot_class\n","\n","    \n","# read from TFRecords. For optimal performance, read from multiple\n","# TFRecord files at once and set the option experimental_deterministic = False\n","# to allow order-altering optimizations.\n","\n","option_no_order = tf.data.Options()\n","option_no_order.experimental_deterministic = False\n","\n","train_path = tf.io.gfile.glob(tfrecord_train_dir+ \"*.tfrec\")\n","val_path = tf.io.gfile.glob(tfrecord_val_dir + \"*.tfrec\")\n","\n","training_dataset = tf.data.TFRecordDataset(train_path, num_parallel_reads=AUTO)\n","training_dataset = training_dataset.with_options(option_no_order)\n","training_dataset = training_dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n","training_dataset = training_dataset.batch(BATCH_SIZE)\n","\n","val_dataset = tf.data.TFRecordDataset(val_path, num_parallel_reads=AUTO)\n","val_dataset = val_dataset.with_options(option_no_order)\n","val_dataset = val_dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n","val_dataset = val_dataset.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bF8cUTL5xBLt","outputId":"c7ff34ee-4dd4-489a-b80f-4b7a135eda7e"},"source":["for image, label in training_dataset.take(1):\n","    print(image.numpy().shape, label.numpy().shape)\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["(128, 224, 224, 3) (128, 42)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MtOeukHQxBLv","outputId":"0d285705-5e2d-4675-ab8b-71b998b59d44"},"source":["for image, label in val_dataset.take(1):\n","    print(image.numpy().shape, label.numpy().shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(128, 224, 224, 3) (128, 42)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UPz3-99bxBLv"},"source":["# READ TEST TFRECORDS"]},{"cell_type":"code","metadata":{"id":"DhkI7ZHgxBLw"},"source":["IMAGE_SIZE = [224,224]\n","BATCH_SIZE = 128\n","\n","def read_tfrecord(example):\n","    features = {\n","        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n","    }\n","    \n","    feature = tf.io.parse_single_example(example, features)\n","    image = tf.image.decode_jpeg(feature['image'], channels=3)\n","    image = tf.image.convert_image_dtype(image, tf.float32)\n","    image = tf.image.resize(image, [*IMAGE_SIZE])\n","    return image\n","\n","    \n","# read from TFRecords. For optimal performance, read from multiple\n","# TFRecord files at once and set the option experimental_deterministic = False\n","# to allow order-altering optimizations.\n","\n","option_no_order = tf.data.Options()\n","option_no_order.experimental_deterministic = False\n","\n","test_path = tf.io.gfile.glob(tfrecord_test_dir+ \"*.tfrec\")\n","\n","test_dataset = tf.data.TFRecordDataset(test_path, num_parallel_reads=AUTO)\n","test_dataset = test_dataset.with_options(option_no_order)\n","test_dataset = test_dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n","test_dataset = test_dataset.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-yWQEpGxBLx","outputId":"86fe65f5-0cbf-4380-8dcf-eda2787ac6c8"},"source":["for image in test_dataset.take(1):\n","    print(image.numpy().shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(128, 224, 224, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TlfG0xU9xBLx"},"source":[""],"execution_count":null,"outputs":[]}]}