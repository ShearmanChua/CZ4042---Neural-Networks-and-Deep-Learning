{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3331,
     "status": "ok",
     "timestamp": 1604371603259,
     "user": {
      "displayName": "null null",
      "photoUrl": "",
      "userId": "04177996287264146405"
     },
     "user_tz": -480
    },
    "id": "-pZ9X8382zfg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "from PIL import Image\n",
    "import os\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSXWniTG2zfk"
   },
   "outputs": [],
   "source": [
    "def _bytestring_feature(list_of_bytestrings):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
    "\n",
    "def _int_feature(list_of_ints): # int64\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
    "\n",
    "def _float_feature(list_of_floats): # float32\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27028,
     "status": "ok",
     "timestamp": 1604371671896,
     "user": {
      "displayName": "null null",
      "photoUrl": "",
      "userId": "04177996287264146405"
     },
     "user_tz": -480
    },
    "id": "A4KeNstYACIf",
    "outputId": "2facac11-8934-43a9-b692-9edb94a2ea0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "executionInfo": {
     "elapsed": 1354,
     "status": "error",
     "timestamp": 1604371675498,
     "user": {
      "displayName": "null null",
      "photoUrl": "",
      "userId": "04177996287264146405"
     },
     "user_tz": -480
    },
    "id": "trCkZwsG2zfm",
    "outputId": "e492ea43-4a6b-4d0a-cfb9-fb0f22f380ec"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-23bc6ffa1cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'df.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('df.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df.make_id = label_encoder.fit_transform(df.make_id)\n",
    "make_id_mapping = label_encoder.classes_\n",
    "df.model_id = label_encoder.fit_transform(df.model_id)\n",
    "model_id_mapping = label_encoder.classes_\n",
    "\n",
    "# Train - 70%\n",
    "# Val - 20%\n",
    "# Test - 10%\n",
    "\n",
    "train_data, val_data = train_test_split(df, train_size=0.80, random_state=42, shuffle=True, stratify=df[['make_id', 'model_id']])\n",
    "train_data, test_data = train_test_split(train_data, train_size=0.875, random_state=42, shuffle=True, stratify=train_data[['make_id', 'model_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3691,
     "status": "ok",
     "timestamp": 1604371732041,
     "user": {
      "displayName": "null null",
      "photoUrl": "",
      "userId": "04177996287264146405"
     },
     "user_tz": -480
    },
    "id": "GNrdMpEFAYm3"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/CZ4042 Project/df.csv')\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df.make_id = label_encoder.fit_transform(df.make_id)\n",
    "make_id_mapping = label_encoder.classes_\n",
    "df.model_id = label_encoder.fit_transform(df.model_id)\n",
    "model_id_mapping = label_encoder.classes_\n",
    "\n",
    "# Train - 70%\n",
    "# Val - 20%\n",
    "# Test - 10%\n",
    "\n",
    "train_data, val_data = train_test_split(df, train_size=0.80, random_state=42, shuffle=True, stratify=df[['make_id', 'model_id']])\n",
    "train_data, test_data = train_test_split(train_data, train_size=0.875, random_state=42, shuffle=True, stratify=train_data[['make_id', 'model_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qn1ry9rMbM_6"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtbF-_ZaTxIe"
   },
   "outputs": [],
   "source": [
    "train_image_paths = train_data['filename']\n",
    "train_labels = train_data[['make_id', 'model_id']]\n",
    "\n",
    "val_image_paths = val_data['filename']\n",
    "val_labels = val_data[['make_id', 'model_id']]\n",
    "\n",
    "test_image_paths = test_data['filename']\n",
    "\n",
    "tfrecord_train_dir = 'tfrecords/train/'\n",
    "tfrecord_val_dir = 'tfrecords/val/'\n",
    "tfrecord_test_dir = 'tfrecords/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrtaQVlF2zfr"
   },
   "source": [
    "## TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "046HIlnZ2zfr",
    "outputId": "4bb9b687-354c-4365-df64-e65b7483719c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matches 95711 images which will be rewritten as 64 .tfrec files containing 1496 images each.\n"
     ]
    }
   ],
   "source": [
    "SHARDS = 64\n",
    "nb_images = len(train_data)\n",
    "shard_size = math.ceil(1.0 * nb_images / SHARDS)\n",
    "print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEP1WVSl2zfu"
   },
   "outputs": [],
   "source": [
    "def _parse_function(filename, label):\n",
    "    img_raw = tf.io.read_file(filename)\n",
    "    return img_raw, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZHqpjZB2zfw"
   },
   "outputs": [],
   "source": [
    "files = tf.data.Dataset.from_tensor_slices((train_image_paths, train_labels))\n",
    "dataset = files.map(_parse_function)\n",
    "dataset = dataset.batch(shard_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXjI7hV2UnaB"
   },
   "outputs": [],
   "source": [
    "def to_tfrecord(tfrec_filewriter, img_bytes, label):\n",
    "    one_hot_class = [np.eye(163)[label[0]], np.eye(1716)[label[1]]]\n",
    "    \n",
    "    feature = {\n",
    "        \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
    "        \"make_id\": _int_feature([label[0]]),\n",
    "        \"make_id_oh\": _float_feature(one_hot_class[0].tolist()),\n",
    "        \"model_id\": _int_feature([label[1]]),\n",
    "        \"model_id_oh\": _float_feature(one_hot_class[1].tolist())\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eW5FZ5BU2zf1",
    "outputId": "bd3a570c-cd33-499e-b457-ba8dfb10f98a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing TFRecords\n",
      "Wrote file tfrecords/train/00-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/01-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/02-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/03-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/04-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/05-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/06-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/07-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/08-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/09-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/10-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/11-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/12-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/13-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/14-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/15-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/16-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/17-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/18-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/19-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/20-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/21-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/22-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/23-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/24-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/25-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/26-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/27-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/28-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/29-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/30-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/31-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/32-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/33-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/34-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/35-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/36-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/37-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/38-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/39-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/40-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/41-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/42-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/43-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/44-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/45-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/46-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/47-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/48-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/49-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/50-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/51-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/52-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/53-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/54-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/55-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/56-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/57-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/58-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/59-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/60-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/61-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/62-1496.tfrec containing 1496 records\n",
      "Wrote file tfrecords/train/63-1463.tfrec containing 1463 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing TFRecords\")\n",
    "for shard, (image, label) in enumerate(dataset):\n",
    "  # batch size used as shard size here\n",
    "  shard_size = image.numpy().shape[0]\n",
    "  # good practice to have the number of records in the filename\n",
    "  filename = tfrecord_train_dir + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n",
    "  \n",
    "  with tf.io.TFRecordWriter(filename) as out_file:\n",
    "    for i in range(shard_size):\n",
    "        example = to_tfrecord(out_file,\n",
    "                              image.numpy()[i],\n",
    "                              label.numpy()[i])\n",
    "        out_file.write(example.SerializeToString())\n",
    "    \n",
    "    print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk9Mm3C92zf4"
   },
   "source": [
    "## VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rm3SVshd2zf5",
    "outputId": "ba1b554f-4a2a-4b3f-d8b8-92c77888e5a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matches 27347 images which will be rewritten as 16 .tfrec files containing 1710 images each.\n"
     ]
    }
   ],
   "source": [
    "SHARDS = 16\n",
    "nb_images = len(val_data)\n",
    "shard_size = math.ceil(1.0 * nb_images / SHARDS)\n",
    "print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNewP5p42zf7"
   },
   "outputs": [],
   "source": [
    "# def _parse_function(filename, label):\n",
    "#     img_raw = tf.io.read_file(filename)\n",
    "#     return img_raw, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-f90YZ02zf-"
   },
   "outputs": [],
   "source": [
    "files = tf.data.Dataset.from_tensor_slices((val_image_paths, val_labels))\n",
    "dataset = files.map(_parse_function)\n",
    "dataset = dataset.batch(shard_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUctTSej2zgA"
   },
   "outputs": [],
   "source": [
    "# def to_tfrecord(tfrec_filewriter, img_bytes, label):\n",
    "#     one_hot_class = np.eye(42)[label] \n",
    "#     feature = {\n",
    "#         \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
    "#         \"class\": _int_feature([label]),\n",
    "#         \"one_hot_class\": _float_feature(one_hot_class.tolist())\n",
    "#     }\n",
    "#     return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YZGObJ12zgC",
    "outputId": "64258d2b-aa1e-458e-c3c2-2bdbaa0b217c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing TFRecords\n",
      "Wrote file tfrecords/val/00-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/01-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/02-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/03-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/04-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/05-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/06-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/07-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/08-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/09-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/10-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/11-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/12-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/13-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/14-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/val/15-1697.tfrec containing 1697 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing TFRecords\")\n",
    "for shard, (image, label) in enumerate(dataset):\n",
    "  # batch size used as shard size here\n",
    "  shard_size = image.numpy().shape[0]\n",
    "  # good practice to have the number of records in the filename\n",
    "  filename = tfrecord_val_dir + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n",
    "  \n",
    "  with tf.io.TFRecordWriter(filename) as out_file:\n",
    "    for i in range(shard_size):\n",
    "        example = to_tfrecord(out_file,\n",
    "                              image.numpy()[i],\n",
    "                              label.numpy()[i])\n",
    "        out_file.write(example.SerializeToString())\n",
    "    \n",
    "    print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzx5gXFS2zgE"
   },
   "source": [
    "## TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_kfvqd2zgF",
    "outputId": "8e8a25bf-9016-4b0f-9aba-7696c91607b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern matches 13674 images which will be rewritten as 8 .tfrec files containing 1710 images each.\n"
     ]
    }
   ],
   "source": [
    "SHARDS = 8\n",
    "nb_images = len(test_data)\n",
    "shard_size = math.ceil(1.0 * nb_images / SHARDS)\n",
    "print(\"Pattern matches {} images which will be rewritten as {} .tfrec files containing {} images each.\".format(nb_images, SHARDS, shard_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvsa1rJD2zgH"
   },
   "outputs": [],
   "source": [
    "def _parse_function(filename):\n",
    "    img_raw = tf.io.read_file(filename)\n",
    "    return img_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oJmbTV32zgI"
   },
   "outputs": [],
   "source": [
    "files = tf.data.Dataset.from_tensor_slices((test_image_paths))\n",
    "dataset = files.map(_parse_function)\n",
    "dataset = dataset.batch(shard_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbbfybMv2zgL"
   },
   "outputs": [],
   "source": [
    "def to_tfrecord(tfrec_filewriter, img_bytes):\n",
    "    feature = {\n",
    "        \"image\": _bytestring_feature([img_bytes]), # one image in the list\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvGWlm1O2zgN",
    "outputId": "7c03f360-bba7-4ba2-aad9-15ebcd8a2d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing TFRecords\n",
      "Wrote file tfrecords/test/00-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/test/01-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/test/02-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/test/03-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/test/04-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/test/05-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/test/06-1710.tfrec containing 1710 records\n",
      "Wrote file tfrecords/test/07-1704.tfrec containing 1704 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing TFRecords\")\n",
    "for shard, (image) in enumerate(dataset):\n",
    "  # batch size used as shard size here\n",
    "  shard_size = image.numpy().shape[0]\n",
    "  # good practice to have the number of records in the filename\n",
    "  filename = tfrecord_test_dir + \"{:02d}-{}.tfrec\".format(shard, shard_size)\n",
    "  \n",
    "  with tf.io.TFRecordWriter(filename) as out_file:\n",
    "    for i in range(shard_size):\n",
    "        example = to_tfrecord(out_file,\n",
    "                              image.numpy()[i])\n",
    "        out_file.write(example.SerializeToString())\n",
    "    \n",
    "    print(\"Wrote file {} containing {} records\".format(filename, shard_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r5pmXzH2zgP"
   },
   "source": [
    "## READ TRAIN/VAL TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsDa8FPI2zgQ"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224,224]\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n",
    "        \"make_id\": tf.io.FixedLenFeature([], tf.int64),   # shape [] means scalar\n",
    "        \"make_id_oh\": tf.io.VarLenFeature(tf.float32) # a certain number of floats\n",
    "        \"model_id\": tf.io.FixedLenFeature([], tf.int64),   # shape [] means scalar\n",
    "        \"model_id_oh\": tf.io.VarLenFeature(tf.float32)# a certain number of floats\n",
    "    }\n",
    "    \n",
    "    feature = tf.io.parse_single_example(example, features)\n",
    "    image = tf.image.decode_jpeg(feature['image'], channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [*IMAGE_SIZE])\n",
    "    label = feature['class']\n",
    "    one_hot_class = tf.sparse.to_dense(feature['one_hot_class'])\n",
    "    one_hot_class = tf.reshape(one_hot_class, [42])\n",
    "    return image, one_hot_class\n",
    "\n",
    "    \n",
    "# read from TFRecords. For optimal performance, read from multiple\n",
    "# TFRecord files at once and set the option experimental_deterministic = False\n",
    "# to allow order-altering optimizations.\n",
    "\n",
    "option_no_order = tf.data.Options()\n",
    "option_no_order.experimental_deterministic = False\n",
    "\n",
    "train_path = tf.io.gfile.glob(tfrecord_train_dir+ \"*.tfrec\")\n",
    "val_path = tf.io.gfile.glob(tfrecord_val_dir + \"*.tfrec\")\n",
    "\n",
    "training_dataset = tf.data.TFRecordDataset(train_path, num_parallel_reads=AUTO)\n",
    "training_dataset = training_dataset.with_options(option_no_order)\n",
    "training_dataset = training_dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "training_dataset = training_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.TFRecordDataset(val_path, num_parallel_reads=AUTO)\n",
    "val_dataset = val_dataset.with_options(option_no_order)\n",
    "val_dataset = val_dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJwNKEHF2zgS",
    "outputId": "fa7a6d24-a19e-4395-e4c8-ce564a8ea0b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 224, 224, 3) (128, 42)\n"
     ]
    }
   ],
   "source": [
    "for image, label in training_dataset.take(1):\n",
    "    print(image.numpy().shape, label.numpy().shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enI5UaAk2zgT",
    "outputId": "274ac14e-6df8-47bc-c040-b17f59284183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 224, 224, 3) (128, 42)\n"
     ]
    }
   ],
   "source": [
    "for image, label in val_dataset.take(1):\n",
    "    print(image.numpy().shape, label.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFO-yzAh2zgV"
   },
   "source": [
    "# READ TEST TFRECORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqkSVNor2zgW"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224,224]\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n",
    "    }\n",
    "    \n",
    "    feature = tf.io.parse_single_example(example, features)\n",
    "    image = tf.image.decode_jpeg(feature['image'], channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, [*IMAGE_SIZE])\n",
    "    return image\n",
    "\n",
    "    \n",
    "# read from TFRecords. For optimal performance, read from multiple\n",
    "# TFRecord files at once and set the option experimental_deterministic = False\n",
    "# to allow order-altering optimizations.\n",
    "\n",
    "option_no_order = tf.data.Options()\n",
    "option_no_order.experimental_deterministic = False\n",
    "\n",
    "test_path = tf.io.gfile.glob(tfrecord_test_dir+ \"*.tfrec\")\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(test_path, num_parallel_reads=AUTO)\n",
    "test_dataset = test_dataset.with_options(option_no_order)\n",
    "test_dataset = test_dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF4nRPYj2zgY",
    "outputId": "27fe62fd-d6e9-4ca5-f125-14de6b90862d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "for image in test_dataset.take(1):\n",
    "    print(image.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqgyNi8e2zgc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tfrecords_handler.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
